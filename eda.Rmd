---
title       : Initial Lexical Analysis of Text Data
subtitle    : 
author      : Jon Leete
---

## Executive Summary

This document summarizes the initial data exploration of the
text data provided for the Coursera Data Science Capstone project.

## Analysis

With each input file, we:

* tokenize the data into words and 2-grams; and,
* count the distinct uses of each word / 2-gram in each file.

For the aggregated data, we:

* total the uses for each word or 2-gram;
* sort the words or 2-grams, most commonly-used first;
* plot the counts; and,
* determine how many we need to meet various fractions of word usage.

### Configuration

We configure our analysis with the following parameters:

```{r analysisParameters, echo=TRUE,message=FALSE, warning=FALSE}
# Use just words, or allow numbers and currency as tokens?
tokenize.words.only = TRUE

# Data file selection parameters
data.dir = "~/r/capstone/data"
locale = "en_US"

# What fractions of word coverage to analyze for
coverage.fractions = c(0.5, 0.9, 0.95, 0.98, 0.99, 0.999)
# Fraction of word uses (vs. distinct words) we want to find
desired.coverage.fraction = 0.98

# Use test data, or "real" data, as below?
test.configuration = FALSE

# Specify data files and fraction of data to sample for analysis
if (test.configuration) {
    data.sources <- c("blogtest", "newstest", "twittertest")
    sample.fraction = 1.0
} else {
    data.sources <- c("blogs", "news", "twitter")
    sample.fraction = 0.00002
}
```

```{r setup, echo=TRUE,message=FALSE, warning=FALSE}
# Setup
library(stringi)
library(plyr)
set.seed(314159)
```

<br>
```{r testWarning, echo=FALSE,message=FALSE, warning=FALSE, results='asis'}
if (test.configuration) {
    print("## WARNING: THIS IS A TEST CONFIGURATION ##")
}
```

```{r functionDeclarations, echo=FALSE,message=FALSE, warning=FALSE}
# Regular expression denoting a word or equivalent that we want to treat as a single token.
token.re <- (
    function() {
        word.re     <- "(?:[:alpha:]+(?:\\.[:alpha:]?|\\'[:alpha:]+)*)"
        number.re   <- "(?:[:digit:]{1,3}(?:(?:(?:\\,[:digit:]{3})|[:digit:]{1,2})*(?:\\.[:digit:]+)?))"
        currency.re <- paste("(?:\\$", number.re, "[MB]?", ")", sep = "")
        
        if (tokenize.words.only) {
            re <- word.re
        } else {
            re <- paste(word.re, number.re, currency.re, sep = "|")
        }
        
        re
    })()

tokenize <- function(line) {
    # Convert a line of text into a vector of tokens.
    #
    # parameters:
    #   line:     the line of text (characters) to tokenize
    # returns: a character vector populated with tokens.
    tokens <- stri_match_all(line, regex = token.re, extended = TRUE)
    tokens <- lapply(tokens[[1]], normalize.token)
    
    tokens
}

normalize.token <- function(s) {
    # Remove a final period from words that don't look like abbreviations
    # (that have a vowel in them).
    
    if (grepl(".*[aeiouAEIOU].*\\.$", s)) {
        toupper(substring(s, 1, nchar(s) - 1))
    } else {
        toupper(s)
    }
}

populate.word.df <- function(src, word, word.df) {
    # Add the word to the word data frame.
    
    i <- which(word.df$word == word)
    
    if (length(i) == 0) {
        # Word not previously found: add row
        old.len = nrow(word.df)
        if (is.null(old.len)) {
            old.len = 0;
        }
        new.len <- old.len + 1
        current.width <- length(word.df)
        
        word.df[new.len, 1] <- word
        word.df[new.len, 2:current.width] <- integer(current.width - 1)
        word.df[new.len, current.width] <- 1
    } else {
        # Word previously found: increment
        word.df[i, src] <- word.df[i, src] + 1
    }
    
    word.df
}

populate.ngram.df <- function(src, tokens, i, n, df) {
    # Add the appropriate n-gram (tokens i and i-1) to the n-gram data frame.
    
    if (i >= n) {
        first <- i - n + 1
        word <- paste(tokens[first:i], collapse = " ")
        
        i <- which(df$word == word)
        
        if (length(i) == 0) {
            # Word not previously found: add row
            old.len = nrow(df)
            if (is.null(old.len)) {
                old.len = 0;
            }
            new.len <- old.len + 1
            current.width <- length(df)
            
            df[new.len, 1] <- word
            df[new.len, 2:current.width] <- integer(current.width - 1)
            df[new.len, current.width] <- 1
        } else {
            # Word previously found: increment
            df[i, src] <- df[i, src] + 1
        }
    }
    
    df
}

populate.df <- function(src, con, sample.fraction = 1.0, df, n = 1) {
    # Filter the specified input file, populating the specified data frame
    # with word or n-gram counts. Use randomly-selected lines in the input
    # file (with probability sample.fraction). All tokens are upper case.
    
    # Tokenize randomly selected lines, add tokens to data frames
    while (length(line <- readLines(con, n = 1, warn = FALSE)) > 0) {
        rand <- rbinom(n = 1, size = 1, prob = sample.fraction)
        
        if (rand == 1) {
            tokens <- tokenize(line)
            
            for (i in 1:length(tokens)) {
                if (n == 1) {
                    df <- populate.word.df(src, tokens[i], df)
                } else {
                    df <- populate.ngram.df(src, tokens, i, n, df)
                }
            }
        }
    }
    
    df
}

create.df <- function(first.col.name, srcs) {
    # Create a data frame with the given first column name,
    # plus a column for each source.
    
    all.colnames <- append(srcs, first.col.name, after = 0)
    df <- read.table(text = "", col.names = all.colnames, stringsAsFactors = FALSE)
    
    df
}

build.df <- function(srcs, sample.fraction = 1.0, dir = ".", locale = "en_US", df, n = 1) {
    # Populate the word or ngram data frames from files
    # for each of the sources specified.
    
    for (src in srcs) {
        filename <- paste(locale, src, "txt", sep = ".")
        path <- paste(dir, filename, sep = "/")
        con <- file(path, "r")
        
        df <- populate.df(src, con, sample.fraction, df, n)
        
        close(con)
    }

    # Calculate total uses across all sources, order by total (descending)
    df$total <- rowSums(df[,2:length(df)])
    df <- df[order(-df$total), ]
    
    # Add cumulative total uses column
    df <- mutate(df, cumsum = cumsum(total))
    
    df
}

line.count <- function(src, dir, locale) {
    filename <- paste(locale, src, "txt", sep = ".")
    path <- paste(dir, filename, sep = "/")
    
    con <- file(path, "r")
    lines <- readLines(con)
    close(con)
    
    length(lines)
}

how.many.needed <- function(fraction, cumulative, round.up = TRUE) {
    # How many words do you need to capture some fraction of
    # all uses?
    total.tokens <- cumulative[length(cumulative)]
    most.tokens  <- total.tokens * fraction
    highest.needed.row <- min(which(cumulative > most.tokens))
    
    highest.needed.row
}
```

### Tokenization

Our tokenization function uses the following regular expression:

```{r showTokenRegex, echo=FALSE,message=FALSE, warning=FALSE}
token.re
```

This expression scans for
```r ifelse(tokenize.words.only, "words only", "numbers, currency and words")```.
Contractions are allowed, and counted as single tokens, if they
include an apostrophe followed by more letters; this pattern may repeat, as
in "shouldn't've." If a word ends in a period, it is considered an abbrevation
if it has no vowels; the period is included as part of the token. (This is
somewhat arbitrary, but covers cases such as "Ms." and "Rd.")

We do _not_ scan for entities such as "New York," as entity recognition is a
complex field in itself, and simple efforts are quite error-prone.

For this analysis, we have scanned and tokenized about ```r sample.fraction * 100```%
of the lines in each file, selected randomly.

### Counting

As we break each input lines into a vector of tokens, we build two data frames:
one counting how many times each word is used in a data source,
the other counting 2-grams. (We store 2-grams internally as words with a space,
as our tokenization doesn't allow spaces in words. If we allow spaces, for instance
to incorporate entity recognition, we would update this accordingly.)

At the end of the counting process, we build a "total" column reflecting the
total number of times each word or 2-gram is used in all the sources.
(At this point, we have no reason not to simply group them all together.)

### Analysis

This section presents basic statistics about the data set:

* Word and line counts from each source
* Distinct word usages
* What fraction of words or 2-grams we need to cover various fractions of the data set

First, let's find basic statistics about each file.

Line counts (for data directory "```r data.dir```", locale "```r locale```"):

```{r countLines, echo=FALSE,message=FALSE, warning=FALSE}
sapply(data.sources, FUN = line.count, dir = data.dir, locale = locale)
```

For word counts, we go ahead and perform the full tokenization, which we can
sum the results of:

```{r runAnalysis, echo=TRUE,message=FALSE, warning=FALSE}
# Initialize single-word and n-gram data frames
word.df <- create.df("word", data.sources)
ngram.df <- create.df("word", data.sources)

# Build data frames
word.df  <- build.df(data.sources, sample.fraction, data.dir, locale, word.df, 1)
ngram.df <- build.df(data.sources, sample.fraction, data.dir, locale, ngram.df, 2)
```

Total word (token) counts:

```{r totalWordCounts, echo=FALSE,message=FALSE, warning=FALSE}
sapply(word.df[,data.sources], FUN = sum, simplify = "array")
```

The combined sources include ```r length(word.df$word)``` distinct words.

Let's take a quick glance at our data, just as a sanity check. First the
single-word data, then the 2-gram data:

```{r illustrateData, echo=FALSE,message=FALSE, warning=FALSE}
head(word.df)
head(ngram.df)
```

The following plots show clearly that most text comprises a surprisingly small
number of words. The plots have been rescaled to show a bit more detail than
would be apparent if the highest word frequencies were on the plot.

```{r plotWordAnalysis, echo=FALSE,message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=8}
max.y.rowindex <- max(floor(length(word.df$word) / 100), 1)
max.y <- word.df[max.y.rowindex, "total"]

par(mfrow = c(1, 2), mai=c(1, 1, 0.2, 0.5))

plot(word.df[,"total"], type = "l", ylim = c(0, max.y), col = 1, ylab = "Word Use Count")
for (i in 2:(length(word.df) - 1)) {
    lines(word.df[,i], col = i - 1)
}

max.y.rowindex <- max(floor(length(ngram.df$word) / 100), 1)
max.y <- ngram.df[max.y.rowindex, "total"]

plot(ngram.df[,"total"], type = "l", ylim = c(0, max.y), col = 1, ylab = "2-Gram Use Count")
for (i in 2:(length(ngram.df) - 1)) {
    lines(ngram.df[,i], col = i)
}
```

The top line is the total number of uses from all sources. One can see that
the various data sources have generally similar word frequencies.

The fraction of words needed to cover a given number of uses is:

```{r coverage, echo=FALSE,message=FALSE, warning=FALSE}
distinct.words = length(word.df$word)

needed.words <- sapply(coverage.fractions, FUN = how.many.needed, cumulative = word.df$cumsum, simplify = "array")

coverage = data.frame(coverage.fraction = coverage.fractions, needed.words = needed.words)
coverage
```

## Conclusion

With our simple tokenization scheme, we found ```r length(word.df$word)```
distinct words, of which we needed
```r needed.words[which(abs(coverage.fractions - desired.coverage.fraction) < 0.00001)]```
to capture ```r desired.coverage.fraction * 100.0```% of the words. This information
will help us optimize our data set for our predictive model.

<br><br><br><br>

## Appendix (Code)

The functions used in the analysis above are defined as follows:
```{r showFunctions, echo=TRUE,message=FALSE, warning=FALSE}
print(tokenize)
print(normalize.token)
print(populate.word.df)
print(populate.ngram.df)
print(populate.df)
print(create.df)
print(build.df)
print(line.count)
print(how.many.needed)
```