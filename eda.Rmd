---
title       : Initial Lexical Analysis of Text Data
subtitle    : 
author      : Jon Leete
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : []            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

## Executive Summary

This document summarizes the initial data exploration of the
text data provided for the Coursera Data Science Capstone project.

## Analysis

uses stringi package (others?)

tokenization
 assumptions
 process

counting
single words: process, summary tables, exploratory graphs
2-grams: same



```{r execSummPlot, echo=FALSE, message=FALSE, fig.height=6, fig.width=12, fig.align='center'}
set.seed(10001)
N <- 10000
sides <- 6

# Uniform (just 1 die)
df <- NULL
v1 <- rep(0, N)
dice <- 1

for (i in 1:N) {
    v1[i] <- trunc(runif(n = dice, min = 1, max = sides + 1))
}

# Vaguely normal: 3 dice
df <- NULL
v3 <- rep(0, N)
dice <- 3

for (i in 1:N) {
    r <- runif(n = dice, min = 1, max = sides + 1)
    c <- trunc(r)
    df <- rbind(df, c)
}
for (i in 1:N) {
    v3[i] <- mean(df[i, ])
}

# Much more normal: 15 dice
df <- NULL
v15 <- rep(0, N)
dice <- 15

for (i in 1:N) {
    r <- runif(n = dice, min = 1, max = sides + 1)
    c <- trunc(r)
    df <- rbind(df, c)
}

for (i in 1:N) {
    v15[i] <- mean(df[i, ])
}

# Histogram breaks for not-very-normal appearance
breaks1 <- seq(from = 0.5, to = 6.5, by = 1)
breaks3 <- seq(from = 0.25, to = 6.75, by = 0.5)

par(mfrow=c(1, 3))
hist(v1, main="One Die (Uniform)", breaks=breaks1, xlab="Value of Die", col="red")
hist(v3, main="Three Dice (Somewhat Normal)", breaks=breaks3, xlab="Mean Die Roll", col="red")
hist(v15, main="Fifteen Dice (Close to Normal)", xlab="Mean Die Roll", col="red")
```

The application discussed herein lets the user experiment with
this effect dynamically.

---

## Application

This application illustrates the Central Limit Theorem,
introduced early in the Coursera Data Science track,
by showing that the means of a random uniform distribution
converge to a normal distribution as the sample size
grows.

The application generates random samples of uniformly-distributed
random numbers. The user selects the sample sizes and number of
samples; it is up to the user to observe the phenomemon
for him/herself by varying these parameters.

![200](assets/img/app.png)

The application is available to run at: https://jleete97.shinyapps.io/normalhist

and the GitHub repository for the code is at: https://github.com/jleete97/normalhist

---

## Theory: Uniform Distribution

Let's simulate rolling one die many times, and look at the distribution of results:

```{r oneDie, echo=TRUE, message=FALSE, warning=FALSE, fig.height=2, fig.width=12, fig.align='center'}
set.seed(10002); v1 <- rep(0, 1000)
for (i in 1:1000) {
    v1[i] <- trunc(runif(n = 1, min = 1, max = 6 + 1))
}
```

```{r oneDieHist, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.5, fig.width=7, fig.align='center'}
# Histogram breaks for not-very-normal appearance
breaks <- seq(from = 0.5, to = 6.5, by = 1)

hist(v1, main="One Die (Uniform)", xlab="Single Die Roll", breaks=breaks, col="red")
```

Unsurprisingly, the distribution looks roughly uniform. Each result, 1-6, comes up
about 1/6 of the time.

---

## Theory: Distribution with Many More Samples

With 15 dice, the results are much more normally distributed,
as the Central Limit Theorem would suggest.

```{r fifteenDice, message=FALSE, warning=FALSE, fig.height=2, fig.width=12, fig.align='center'}
set.seed(10005); v15 <- rep(0, 1000)
for (i in 1:1000) {
    r <- trunc(runif(n = 15, min = 1, max = 6 + 1))
    v15[i] <- mean(trunc(r))
}
```

```{r fifteenDiceHist, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=10, fig.align='center'}
hist(v15, main="Fifteen Dice (Approximately Normal)", breaks=seq(from=1.5, to=5.5, by=0.2), xlab="Mean Die Roll", col="red")
```
