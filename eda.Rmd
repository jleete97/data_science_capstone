---
title       : Initial Lexical Analysis of Text Data
subtitle    : 
author      : Jon Leete
---

## Executive Summary

This document summarizes the initial data exploration of the
text data provided for the Coursera Data Science Capstone project.

## Analysis

### Configuration

We configure our analysis with the following parameters:

```{r analysisParameters, echo=TRUE}
tokenize.words.only = TRUE
sample.fraction = 0.001

data.dir = "data"
locale = "en_US"

data.sources <- c("test")
```

Their use is explained below.

```{r functionDeclarations, echo=FALSE, message=FALSE}

# Setup
library(stringi)
library(plyr)
set.seed(314159)

# Regular expression denoting a word or equivalent that we want to treat as a single token.
token.re <- (
    function() {
        word.re     <- "(?:[:alpha:]+(?:\\.[:alpha:]?|\\'[:alpha:]+)*)"
        number.re   <- "(?:[:digit:]{1,3}(?:(?:(?:\\,[:digit:]{3})|[:digit:]{1,2})*(?:\\.[:digit:]+)?))"
        currency.re <- paste("(?:\\$", number.re, "[MB]?", ")", sep = "")
        
        if (tokenize.words.only) {
            re <- word.re
        } else {
            re <- paste(word.re, number.re, currency.re, sep = "|")
        }
        
        re
    })()

tokenize <- function(line) {
    # Convert a line of text into a vector of tokens.
    #
    # parameters:
    #   line:     the line of text (characters) to tokenize
    # returns: a character vector populated with tokens.
    tokens <- stri_match_all(line, regex = token.re, extended = TRUE)
    tokens <- lapply(tokens[[1]], normalize.token)
#    tokens <- paste(tokens, collapse = " ")
    
    tokens
}

normalize.token <- function(s) {
    # Remove a final period from words that don't look like abbreviations
    # (that have a vowel in them).
    
    if (grepl(".*[aeiouAEIOU].*\\.$", s)) {
        toupper(substring(s, 1, nchar(s) - 1))
    } else {
        toupper(s)
    }
}

populate.word.df <- function(src, word, word.df) {
    # Add the word to the word data frame.
    
    i <- which(word.df$word == word)
    
    if (length(i) == 0) {
        # Word not previously found: add row
        old.len = nrow(word.df)
        if (is.null(old.len)) {
            old.len = 0;
        }
        new.len <- old.len + 1
        current.width <- length(word.df)
        
        word.df[new.len, 1] <- word
        word.df[new.len, 2:current.width] <- integer(current.width - 1)
        word.df[new.len, current.width] <- 1
    } else {
        # Word previously found: increment
        word.df[i, src] <- word.df[i, src] + 1
    }
    
    word.df
}

populate.ngram.df <- function(src, tokens, i, n, df) {
    # Add the appropriate n-gram (tokens i and i-1) to the n-gram data frame.
    
    if (i >= n) {
        first <- i - n + 1
        word <- paste(tokens[first:i], collapse = " ")
        
        i <- which(df$word == word)
        
        if (length(i) == 0) {
            # Word not previously found: add row
            old.len = nrow(df)
            if (is.null(old.len)) {
                old.len = 0;
            }
            new.len <- old.len + 1
            current.width <- length(df)
            
            df[new.len, 1] <- word
            df[new.len, 2:current.width] <- integer(current.width - 1)
            df[new.len, current.width] <- 1
        } else {
            # Word previously found: increment
            df[i, src] <- df[i, src] + 1
        }
    }
    
    df
}

populate.df <- function(src, con, sample.fraction = 1.0, df, n = 1) {
    # Filter the specified input file, populating the specified data frame
    # with word or n-gram counts. Use randomly-selected lines in the input
    # file (with probability sample.fraction). All tokens are upper case.
    
    # Tokenize randomly selected lines, add tokens to data frames
    while (length(line <- readLines(con, n = 1, warn = FALSE)) > 0) {
        rand <- rbinom(n = 1, size = 1, prob = sample.fraction)
        
        if (rand == 1) {
            tokens <- tokenize(line)
            
            for (i in 1:length(tokens)) {
                if (n == 1) {
                    df <- populate.word.df(src, tokens[i], df)
                } else {
                    populate.ngram.df(src, tokens, i, n, df)
                }
            }
        }
    }
    
    df
}

create.df <- function(first.col.name, srcs) {
    # Create a data frame with the given first column name,
    # plus a column for each source.
    
    all.colnames <- append(srcs, first.col.name, after = 0)
    df <- read.table(text = "", col.names = all.colnames, stringsAsFactors = FALSE)
    
    df
}

build.df <- function(srcs, sample.fraction = 1.0, dir = ".", locale = "en_US", df, n = 1) {
    # Populate the word or ngram data frames from files
    # for each of the sources specified.
    
    for (src in srcs) {
        filename <- paste(locale, src, "txt", sep = ".")
        path <- paste(dir, filename, sep = "/")
        con <- file(path, "r")
        
        df <- populate.df(src, con, sample.fraction, df, n)
        
        close(con)
    }

    # Calculate total uses across all sources, order by total (descending)
    df$total <- rowSums(df[,2:length(df)])
    df <- df[order(-df$total), ]
    
    # Add cumulative total uses column
    df <- mutate(df, cumsum = cumsum(total))
    
    df
}

line.count <- function(src, dir, locale) {
    filename <- paste(locale, src, "txt", sep = ".")
    path <- paste(dir, filename, sep = "/")
    
    con <- file(path, "r")
    lines <- readLines(con)
    close(con)
    
    length(lines)
}

how.many.needed <- function(fraction, cumulative, round.up = TRUE) {
    # How many words do you need to capture some fraction of
    # all uses?
    total.tokens <- cumulative[length(cumulative)]
    most.tokens  <- total.tokens * fraction
    highest.needed.row <- min(which(cumulativ > most.tokens))
    
    highest.needed.row
}
```

We do the following with each input file:

* Tokenize the data into words and 2-grams
* Count the distinct uses of each word / 2-gram in each file

We then total the uses for each word or 2-gram, plot the counts, and determine
how many we need to meet various fractions of word usage.

### Tokenization

Our tokenization function uses the following regular expression:

```{r showTokenRegex}
token.re
```

This expression scans for
```r ifelse(tokenize.words.only, "words only", "numbers, currency and words")```.
A word is considered a contraction, and counted as a single token, if it
includes an apostrophe followed by more letters; this pattern may repeat, as
in "shouldn't've." A word ending in a period is considered an abbrevation, and
the period included as part of the token, if it has no vowels. (This is somewhat
arbitrary, but covers cases such as "Ms." and "St.")

We do _not_ scan for entities such as "New York," as entity recognition is a
difficult field in itself, and simple efforts are quite error-prone.

For this analysis, we have scanned and tokenized about ```r sample.fraction```
of the lines in each file, selected randomly.

### Counting

As we break each input lines into a vector of tokens, we build two data frames:
one counting how many times each word is used in a data source,
the other counting 2-grams. (We store 2-grams internally as words with a space,
as our tokenization doesn't allow spaces in words. If we allow spaces, for instance
to incorporate entity recognition, we would update this accordingly.)

At the end of the counting process, we build a "total" column reflecting the
total number of times each word or 2-gram is used in all the sources.
(At this point, we have no reason not to simply group them all together.)

### Analysis

This section presents basic statistics about the data set:

* Word and line counts from each source
* Distinct word usages
* What fraction of words or 2-grams we need to cover various fractions of the data set

First, let's find basic statistics about each file.

Line counts:

```{r countLines, echo=TRUE}
sapply(srcs, FUN = line.count, dir = data.dir, locale = locale)
```

For word counts, we go ahead and perform the full tokenization, which we can
sum the results of:

```{r runAnalysis, echo=TRUE}
# Initialize single-word and n-gram data frames
word.df <- create.df("word", data.sources)
ngram.df <- create.df("word", data.sources)

# Build data frames
word.df <- build.df(data.sources, sample.fraction, data.dir, locale, word.df, 1)
#ngram.df <- build.df(srcs, sample.fraction, dir, locale, ngram.df, 2)
```

Total word (token) counts:

```{r totalWordCounts, echo=TRUE}
sapply(word.df[,srcs], FUN = sum, simplify = "array")
```

The combined sources include ```r length(word.df[1])``` distinct words.

The following plots show clearly that most text comprises a surprisingly small
number of words. The plots have been rescaled to show a bit more detail than
would be apparent if the highest word frequencies were on the plot.

```{r plotWordAnalysis, echo=FALSE, message=FALSE, fig.align='center', fig.height=2, fig.width=6}
max.y.rowindex <- max(floor(length(word.df$word) / 100), 1)
max.y <- word.df[max.y.rowindex, "total"]

par(mfrow = c(1, 2))

plot(word.df[,2], type = "l", ylim = c(0, max.y), col = 1, ylab = "Word Use Count")
for (i in 3:length(word.df)) {
    lines(word.df[,i], col = i - 1)
}

max.y.rowindex <- max(floor(length(ngram.df$word) / 100), 1)
max.y <- ngram.df[max.y.rowindex, "total"]

plot(ngram.df[,2], type = "l", ylim = c(0, max.y), col = 1, ylab = "2-Gram Use Count")
for (i in 3:length(ngram.df)) {
    lines(ngram.df[,i], col = i - 1)
}
```

The fraction of words needed to cover a given number of uses is:

```{r coverage}
total.tokens <- word.df[length(word.df$word), "cumsum"]
most.tokens  <- total.tokens * 0.90
highest.needed.row <- min(which(word.df$cumsum > most.tokens))

print(paste("need:", highest.needed.row))
word.df
}
```

```{r coverage, echo=TRUE}
distinct.words = length(word.df[1])

coverage.fractions = c(0.5, 0.9, 0.95, 0.99, 0.999)
needed.words <- lapply(coverage.fractions, FUN = (how.many.needed), cumulative = word.df$cumsum)

needed.words
```